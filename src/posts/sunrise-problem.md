---
title: Задача о восходе солнца
---

Взойдёт ли завтра солнце? Странный вопрос, о котором вы вряд ли задумываетесь, ведь оно всходит каждый день. Да и чтобы ответить на этот вопрос,
потребуются обширные астрономические знания о Солнце. Давайте немного поменяем вопрос и посмотрим на него с точки зрения математики, а не
физики: какова вероятность того, что завтра взойдёт солнце? Вопрос всё ещё абсурден, однако на него можно постараться дать ответ.

Здравствуйте, дорогие любители математики и просто те, кого заинтересовала тема этой статьи! В настоящей статье я рассказываю об одной
замечательной задаче — задаче о восходе солнца. С этой задачей я познакомился на кружке по математическим основам информатики, который
проводится в лицее НИУ ВШЭ, и мне очень она понравилась.

Для понимания статьи желательно быть знакомым с основами теории вероятностей, такими как условная и полная вероятности, теорема Байеса и
распределение вероятностей, а также уметь работать с интегралами.

### Немного об истории задачи

Задача о восходе солнца (англ. *sunrise problem*) была представлена в 1814 году великим математиком и физиком Пьером-Симоном
Лапласом в его работе *«Essai philosophique sur les probabilités»*. Формулировка задачи следующая: 

**«Какова вероятность того, что завтра взойдёт солнце?»**

Лаплас решил эту задачу с помощью своего
<a class="text-link" href="https://en.wikipedia.org/wiki/Rule_of_succession" target="_blank">правила последовательности</a> (англ. *rule of succession*), представленного в той же работе, которое гласит, что если провести эксперимент, оканчивающийся либо успехом, либо неудачей, $n$ раз независимо и получить в результате $s$ успехов и $n-s$ неудач, то вероятность того, что следующий эксперимент будет успешным, равна $(s+1) / (n+2)$.

Воспользуемся этим правилом для решения нашей задачи. Пусть солнце всходило $n=10000$ раз, тогда вероятность того, что оно взойдёт
завтра, равна $$\frac{n+1}{n+2} = \frac{10001}{10002} \approx 0.99990002$$ С каждым днём эта вероятность увеличивается, так как
увеличивается $n$. Однако Лаплас заметил, что такая вероятность восхода солнца всё равно кажется малой для нас, и на самом деле неправильно
в этой задаче применять правило последовательности. Действительно, представьте, что солнце не взойдёт завтра с вероятностью 
$1 - 0.99990002 = 0.00009998$. Мы не можем принять то, что есть такой большой шанс, что оно не взойдёт. Так думали и современники Лапласа,
поэтому в его адрес было много критики по поводу такого решения этой задачи. 

На самом деле, конечно, Лаплас не пытался по-настоящему оценить вероятность восхода солнца, это был лишь пример применения его
правила последовательности, однако пример не совсем удачный. Дело в том, что правило последовательности применяется для оценивания вероятности
в тех случаях, когда у нас нет почти никаких априорных знаний о системе, которую мы исследуем, однако в задаче о восходе солнца у нас есть 
априорные знания о том, как восходит солнце, поэтому и получаются описанные выше противоречия.

### Математическая постановка задачи

Давайте теперь сформулируем более полно, математически, правило последовательности. Пусть есть случайные величины 
$\xi_1, \xi_2,..., \xi_{n+1}$, такие, что $\xi_i \sim Bern(p)$ и все $\xi_i$ условно независимы относительно $p$, т.е.
$$P(\xi_i \cap \xi_j\\, |\\, p) = P(\xi_i\\, |\\, p)P(\xi_j\\, |\\, p)$$
Тогда, если больше нам ничего о них не известно, то $$P(\xi_{n+1} = 1\\, |\\, \sum_{i=1}^{n} \xi_i = s) = \frac{s+1}{n+2}$$

В этой формулировке $\xi_1, \xi_2,..., \xi_{n+1}$ — это последовательность экспериментов, каждый из которых завершается успехом 
с вероятностью $p$ и неудачей с вероятностью $1-p$, т.е. случайная величина $\xi_i$ имеет распределение Бернулли с параметром $p$:
$$\xi_i \sim Bern(p) \\, \Leftrightarrow \\, \xi_i =
\left\\{
\begin{array}{l}
1, \\, p \\\\
0, \\, 1-p
\end{array}
\right.
$$

Вся суть правила последовательности заключается в том, мы хотим оценить вероятность успеха следующего эксперимента при том, что мы мало знаем
о системе, которую исследуем. Всё, что нам известно, — это последовательность результатов независимых экспериментов, причём мы знаем, что 
каждый эксперимент может окончиться как успехом, так и неудачей. То есть вероятность $p$, с которой эксперимент оканчивается успехом, мы не
знаем, поэтому можно считать $p$ случайной величиной, принимающей значения на интервале $(0, 1)$. Важный момент здесь в том, что
$p$ — это, вообще говоря, не случайное значение, просто оно нам не известно, именно поэтому мы считаем его случайной величиной.

### Об априорном распределении случайной величины $p$

И тут мы встречаемся с ключевым вопросом: каково априорное распределение случайной величины $p$? Чтобы ответить на этот вопрос, 
необходимо понимать байесовскую интерпретацию вероятности. Она говорит о том, что вероятность — это степень уверенности в истинности суждения.
При получении новой информации об предмете суждения для определения степени уверенности используется теорема Байеса. По сути теорема Байеса 
гласит, что априорную, т.е до получения любой дополнительной информации, вероятность нужно обновлять каждый раз, когда мы получаем новую 
информацию об объекте суждения, тем самым получая уже не априорную, а апостериорную вероятность, т.е. вероятность после получения новой 
информации.

Априорные вероятности могут быть как информативными, в том смысле, что они учитывают какие-то априорные знания о вероятностях событий, так
и не информативными, т.е. учитывающими настолько мало, насколько это возможно.

Но как же измерить «информативность» априорного распределения вероятностей? На помощь приходит теория информации, которая изучает измерение
кол-ва информации, свойства информации и т.д. На самом деле информацию можно воспринимать следующим образом: чем больше информации у вас есть
о каком-то событии, тем больше вы уверены в одном из возможных его исходов, и наоборот. То есть теория вероятностей и теория информации
довольно сильно связаны друг с другом.

Ключевым понятием в теории информации является 
<a class="text-link" href="https://ru.wikipedia.org/wiki/Информационная_энтропия" target="_blank">информационная энтропия</a>. 
Она характеризует меру неопределённости какой-либо системы. Для дискретной случайной величины $X$, принимающей $n$ независимых значений $x_i$, с распределением, заданным функцией вероятности $p_X(x_i) = P(X = x_i)$, информационная энтропия рассчитывается по формуле Шеннона, где 
$I(X) = -\log p_X(X)$ — <a class="text-link" href="https://ru.wikipedia.org/wiki/Собственная_информация" target="_blank">собственная информация</a>
случайной величины $X$: $$H(X) = -\sum_{i=1}^{n} p_X(x_i) \log p_X(x_i) = \mathbb{E}(I(X))$$

Чем больше энтропия распределения вероятностей, тем больше неопределённость, т.е. тем менее информативным является это распределение.
С помощью формулы Шеннона и метода множителей Лагранжа можно 
<a class="text-link" href="https://en.wikipedia.org/wiki/Lagrange_multiplier#Example_3:_Entropy" target="_blank">доказать</a>, что наименее информативным является равномерное распределение вероятностей. Интуитивно это можно понять на примере честной и нечестной монет: если вы подбрасываете честную монетку, у
которой вероятности выпадения орла и решки равны $1/2$, вы менее уверены в каком-либо из исходов, чем когда вы подбрасываете нечестную монетку, у которой
вероятности выпадения орла и решки, скажем, равны $2/3$ и $1/3$ соответственно. Честная монета как раз имеет равномерное распределение вероятностей.

Таким образом, так как в правиле последовательности мы ничего не знаем о вероятности $p$, с которой эксперимент оканчивается успехом, то можно сказать,
что случайная величина $p$ имеет непрерывное равномерное распределение на интервале $(0, 1)$, т.е. $p \sim U(0, 1)$. Такое непрерывное равномерное
распределение называют стандартным.

### Решение задачи